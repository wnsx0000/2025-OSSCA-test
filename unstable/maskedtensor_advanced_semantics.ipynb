{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# MaskedTensor Advanced Semantics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before working on this tutorial, please make sure to review our\n`MaskedTensor Overview tutorial <https://pytorch.org/tutorials/prototype/maskedtensor_overview.html>`.\n\nThe purpose of this tutorial is to help users understand how some of the advanced semantics work\nand how they came to be. We will focus on two particular ones:\n\n*. Differences between MaskedTensor and [NumPy's MaskedArray](https://numpy.org/doc/stable/reference/maskedarray.html)_  \n*. Reduction semantics\n\n## Preparation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.masked import masked_tensor\nimport numpy as np\nimport warnings\n\n# Disable prototype warnings and such\nwarnings.filterwarnings(action='ignore', category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MaskedTensor vs NumPy's MaskedArray\n\nNumPy's ``MaskedArray`` has a few fundamental semantics differences from MaskedTensor.\n\n*. Their factory function and basic definition inverts the mask (similar to ``torch.nn.MHA``); that is, MaskedTensor\n   uses ``True`` to denote \"specified\" and ``False`` to denote \"unspecified\", or \"valid\"/\"invalid\",\n   whereas NumPy does the opposite. We believe that our mask definition is not only more intuitive,\n   but it also aligns more with the existing semantics in PyTorch as a whole.\n*. Intersection semantics. In NumPy, if one of two elements are masked out, the resulting element will be\n   masked out as well -- in practice, they\n   [apply the logical_or operator](https://github.com/numpy/numpy/blob/68299575d8595d904aff6f28e12d21bf6428a4ba/numpy/ma/core.py#L1016-L1024)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = torch.arange(5.)\nmask = torch.tensor([True, True, False, True, False])\nnpm0 = np.ma.masked_array(data.numpy(), (~mask).numpy())\nnpm1 = np.ma.masked_array(data.numpy(), (mask).numpy())\n\nprint(\"npm0:\\n\", npm0)\nprint(\"npm1:\\n\", npm1)\nprint(\"npm0 + npm1:\\n\", npm0 + npm1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Meanwhile, MaskedTensor does not support addition or binary operators with masks that don't match --\nto understand why, please find the `section on reductions <reduction-semantics>`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mt0 = masked_tensor(data, mask)\nmt1 = masked_tensor(data, ~mask)\nprint(\"mt0:\\n\", mt0)\nprint(\"mt1:\\n\", mt1)\n\ntry:\n    mt0 + mt1\nexcept ValueError as e:\n    print (\"mt0 + mt1 failed. Error: \", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, if this behavior is desired, MaskedTensor does support these semantics by giving access to the data and masks\nand conveniently converting a MaskedTensor to a Tensor with masked values filled in using :func:`to_tensor`.\nFor example:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t0 = mt0.to_tensor(0)\nt1 = mt1.to_tensor(0)\nmt2 = masked_tensor(t0 + t1, mt0.get_mask() & mt1.get_mask())\n\nprint(\"t0:\\n\", t0)\nprint(\"t1:\\n\", t1)\nprint(\"mt2 (t0 + t1):\\n\", mt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the mask is `mt0.get_mask() & mt1.get_mask()` since :class:`MaskedTensor`'s mask is the inverse of NumPy's.\n\n\n## Reduction Semantics\n\nRecall in [MaskedTensor's Overview tutorial](https://pytorch.org/tutorials/prototype/maskedtensor_overview.html)_\nwe discussed \"Implementing missing torch.nan* ops\". Those are examples of reductions -- operators that remove one\n(or more) dimensions from a Tensor and then aggregate the result. In this section, we will use reduction semantics\nto motivate our strict requirements around matching masks from above.\n\nFundamentally, :class:`MaskedTensor`s perform the same reduction operation while ignoring the masked out\n(unspecified) values. By way of example:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = torch.arange(12, dtype=torch.float).reshape(3, 4)\nmask = torch.randint(2, (3, 4), dtype=torch.bool)\nmt = masked_tensor(data, mask)\n\nprint(\"data:\\n\", data)\nprint(\"mask:\\n\", mask)\nprint(\"mt:\\n\", mt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, the different reductions (all on dim=1):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"torch.sum:\\n\", torch.sum(mt, 1))\nprint(\"torch.mean:\\n\", torch.mean(mt, 1))\nprint(\"torch.prod:\\n\", torch.prod(mt, 1))\nprint(\"torch.amin:\\n\", torch.amin(mt, 1))\nprint(\"torch.amax:\\n\", torch.amax(mt, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of note, the value under a masked out element is not guaranteed to have any specific value, especially if the\nrow or column is entirely masked out (the same is true for normalizations).\nFor more details on masked semantics, you can find this [RFC](https://github.com/pytorch/rfcs/pull/27)_.\n\nNow, we can revisit the question: why do we enforce the invariant that masks must match for binary operators?\nIn other words, why don't we use the same semantics as ``np.ma.masked_array``? Consider the following example:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data0 = torch.arange(10.).reshape(2, 5)\ndata1 = torch.arange(10.).reshape(2, 5) + 10\nmask0 = torch.tensor([[True, True, False, False, False], [False, False, False, True, True]])\nmask1 = torch.tensor([[False, False, False, True, True], [True, True, False, False, False]])\nnpm0 = np.ma.masked_array(data0.numpy(), (mask0).numpy())\nnpm1 = np.ma.masked_array(data1.numpy(), (mask1).numpy())\n\nprint(\"npm0:\", npm0)\nprint(\"npm1:\", npm1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's try addition:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"(npm0 + npm1).sum(0):\\n\", (npm0 + npm1).sum(0))\nprint(\"npm0.sum(0) + npm1.sum(0):\\n\", npm0.sum(0) + npm1.sum(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sum and addition should clearly be associative, but with NumPy's semantics, they are not,\nwhich can certainly be confusing for the user.\n\n:class:`MaskedTensor`, on the other hand, will simply not allow this operation since `mask0 != mask1`.\nThat being said, if the user wishes, there are ways around this\n(for example, filling in the MaskedTensor's undefined elements with 0 values using :func:`to_tensor`\nlike shown below), but the user must now be more explicit with their intentions.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mt0 = masked_tensor(data0, ~mask0)\nmt1 = masked_tensor(data1, ~mask1)\n\n(mt0.to_tensor(0) + mt1.to_tensor(0)).sum(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we have learned about the different design decisions behind MaskedTensor and\nNumPy's MaskedArray, as well as reduction semantics.\nIn general, MaskedTensor is designed to avoid ambiguity and confusing semantics (for example, we try to preserve\nthe associative property amongst binary operations), which in turn can necessitate the user\nto be more intentional with their code at times, but we believe this to be the better move.\nIf you have any thoughts on this, please [let us know](https://github.com/pytorch/pytorch/issues)_!\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}