{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# torch.vmap\nThis tutorial introduces torch.vmap, an autovectorizer for PyTorch operations.\ntorch.vmap is a prototype feature and cannot handle a number of use cases;\nhowever, we would like to gather use cases for it to inform the design. If you\nare considering using torch.vmap or think it would be really cool for something,\nplease contact us at https://github.com/pytorch/pytorch/issues/42368.\n\n## So, what is vmap?\nvmap is a higher-order function. It accepts a function `func` and returns a new\nfunction that maps `func` over some dimension of the inputs. It is highly\ninspired by JAX's vmap.\n\nSemantically, vmap pushes the \"map\" into PyTorch operations called by `func`,\neffectively vectorizing those operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n# NB: vmap is only available on nightly builds of PyTorch.\n# You can download one at pytorch.org if you're interested in testing it out.\nfrom torch import vmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first use case for vmap is making it easier to handle\nbatch dimensions in your code. One can write a function `func`\nthat runs on examples and then lift it to a function that can\ntake batches of examples with `vmap(func)`. `func` however\nis subject to many restrictions:\n\n- it must be functional (one cannot mutate a Python data structure\n  inside of it), with the exception of in-place PyTorch operations.\n- batches of examples must be provided as Tensors. This means that\n  vmap doesn't handle variable-length sequences out of the box.\n\nOne example of using `vmap` is to compute batched dot products. PyTorch\ndoesn't provide a batched `torch.dot` API; instead of unsuccessfully\nrummaging through docs, use `vmap` to construct a new function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.dot                            # [D], [D] -> []\nbatched_dot = torch.vmap(torch.dot)  # [N, D], [N, D] -> [N]\nx, y = torch.randn(2, 5), torch.randn(2, 5)\nbatched_dot(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`vmap` can be helpful in hiding batch dimensions, leading to a simpler\nmodel authoring experience.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size, feature_size = 3, 5\nweights = torch.randn(feature_size, requires_grad=True)\n\n# Note that model doesn't work with a batch of feature vectors because\n# torch.dot must take 1D tensors. It's pretty easy to rewrite this\n# to use `torch.matmul` instead, but if we didn't want to do that or if\n# the code is more complicated (e.g., does some advanced indexing\n# shenanigins), we can simply call `vmap`. `vmap` batches over ALL\n# inputs, unless otherwise specified (with the in_dims argument,\n# please see the documentation for more details).\ndef model(feature_vec):\n    # Very simple linear model with activation\n    return feature_vec.dot(weights).relu()\n\nexamples = torch.randn(batch_size, feature_size)\nresult = torch.vmap(model)(examples)\nexpected = torch.stack([model(example) for example in examples.unbind()])\nassert torch.allclose(result, expected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`vmap` can also help vectorize computations that were previously difficult\nor impossible to batch. This bring us to our second use case: batched\ngradient computation.\n\n- https://github.com/pytorch/pytorch/issues/8304\n- https://github.com/pytorch/pytorch/issues/23475\n\nThe PyTorch autograd engine computes vjps (vector-Jacobian products).\nUsing vmap, we can compute (batched vector) - jacobian products.\n\nOne example of this is computing a full Jacobian matrix (this can also be\napplied to computing a full Hessian matrix).\nComputing a full Jacobian matrix for some function f: R^N -> R^N usually\nrequires N calls to `autograd.grad`, one per Jacobian row.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Setup\nN = 5\ndef f(x):\n    return x ** 2\n\nx = torch.randn(N, requires_grad=True)\ny = f(x)\nbasis_vectors = torch.eye(N)\n\n# Sequential approach\njacobian_rows = [torch.autograd.grad(y, x, v, retain_graph=True)[0]\n                 for v in basis_vectors.unbind()]\njacobian = torch.stack(jacobian_rows)\n\n# Using `vmap`, we can vectorize the whole computation, computing the\n# Jacobian in a single call to `autograd.grad`.\ndef get_vjp(v):\n    return torch.autograd.grad(y, x, v)[0]\n\njacobian_vmap = vmap(get_vjp)(basis_vectors)\nassert torch.allclose(jacobian_vmap, jacobian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The third main use case for vmap is computing per-sample-gradients.\nThis is something that the vmap prototype cannot handle performantly\nright now. We're not sure what the API for computing per-sample-gradients\nshould be, but if you have ideas, please comment in\nhttps://github.com/pytorch/pytorch/issues/7786.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def model(sample, weight):\n    # do something...    \n    return torch.dot(sample, weight)\n\ndef grad_sample(sample):\n    return torch.autograd.functional.vjp(lambda weight: model(sample), weight)[1]\n\n# The following doesn't actually work in the vmap prototype. But it\n# could be an API for computing per-sample-gradients.\n\n# batch_of_samples = torch.randn(64, 5)\n# vmap(grad_sample)(batch_of_samples)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}